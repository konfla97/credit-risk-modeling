{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a44d8c-d7a4-4671-a2eb-f6236199f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£ Scorecard (WOE) notebook\n",
    "# Goal: Build an interpretable credit scorecard using manual binning + WOE + Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b40077e-6735-4ea6-9e1a-d9a429276c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Imports and project paths\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIR = Path(\"~/Documents/credit-scoring-home-credit\").expanduser()\n",
    "DATA_PROCESSED = PROJECT_DIR / \"data\" / \"processed\"\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f60466-c652-4f51-8b8a-66179861c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset created in previous notebook\n",
    "\n",
    "clean_path = DATA_PROCESSED / \"application_cleaned.csv\"\n",
    "\n",
    "df_enhanced = pd.read_csv(clean_path)\n",
    "\n",
    "df_enhanced.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb87761-7d22-4207-a01e-a68b6400a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Select a small set of variables for the scorecard\n",
    "\n",
    "scorecard_vars = [\n",
    "    \"EXT_SOURCE_2\",\n",
    "    \"EXT_SOURCE_3\",\n",
    "    \"AMT_CREDIT\",\n",
    "    \"AMT_GOODS_PRICE\",\n",
    "    \"NAME_INCOME_TYPE\",\n",
    "    \"NAME_EDUCATION_TYPE\",\n",
    "    \"OCCUPATION_TYPE\",\n",
    "    \"TARGET\"\n",
    "]\n",
    "\n",
    "df_sc = df_enhanced[scorecard_vars].copy()\n",
    "df_sc.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce17bc7-0f47-42c0-bd16-b4a12e0e6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix missing values in categorical variables (important for WOE)\n",
    "\n",
    "categorical_vars = [\n",
    "    \"NAME_INCOME_TYPE\",\n",
    "    \"NAME_EDUCATION_TYPE\",\n",
    "    \"OCCUPATION_TYPE\"\n",
    "]\n",
    "\n",
    "for col in categorical_vars:\n",
    "    df_sc[col] = df_sc[col].fillna(\"MISSING\")\n",
    "\n",
    "df_sc[categorical_vars].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34482bc4-6dc9-4999-864e-bb9e46ac61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4Ô∏è‚É£ Create quantile bins and handle missing values explicitly\n",
    "\n",
    "numeric_vars = [\"EXT_SOURCE_2\", \"EXT_SOURCE_3\", \"AMT_CREDIT\", \"AMT_GOODS_PRICE\"]\n",
    "\n",
    "for col in numeric_vars:\n",
    "    df_sc[col + \"_BIN\"] = pd.qcut(\n",
    "        df_sc[col],\n",
    "        q=5,\n",
    "        duplicates=\"drop\"\n",
    "    )\n",
    "    \n",
    "    # Convert bins to string\n",
    "    df_sc[col + \"_BIN\"] = df_sc[col + \"_BIN\"].astype(str)\n",
    "    \n",
    "    # Replace 'nan' string with explicit missing label\n",
    "    df_sc.loc[df_sc[col].isna(), col + \"_BIN\"] = \"MISSING\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331550b4-20e2-4ee0-9963-acb4b6b33201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5Ô∏è‚É£ Compute WOE and IV (with Laplace smoothing to avoid infinite values)\n",
    "\n",
    "def compute_woe_iv(df, feature, target=\"TARGET\"):\n",
    "    \n",
    "    grouped = df.groupby(feature)[target]\n",
    "    \n",
    "    summary = grouped.agg(\n",
    "        total=\"count\",\n",
    "        bad=\"sum\"\n",
    "    )\n",
    "    \n",
    "    summary[\"good\"] = summary[\"total\"] - summary[\"bad\"]\n",
    "    \n",
    "    total_bad = summary[\"bad\"].sum()\n",
    "    total_good = summary[\"good\"].sum()\n",
    "    \n",
    "    # Laplace smoothing (prevents infinite WOE)\n",
    "    summary[\"bad_dist\"] = (summary[\"bad\"] + 0.5) / (total_bad + 1)\n",
    "    summary[\"good_dist\"] = (summary[\"good\"] + 0.5) / (total_good + 1)\n",
    "    \n",
    "    summary[\"woe\"] = np.log(summary[\"good_dist\"] / summary[\"bad_dist\"])\n",
    "    \n",
    "    summary[\"iv\"] = (summary[\"good_dist\"] - summary[\"bad_dist\"]) * summary[\"woe\"]\n",
    "    \n",
    "    iv_total = summary[\"iv\"].sum()\n",
    "    \n",
    "    return summary, iv_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f217371-5b55-40f9-8e15-59450daa1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6Ô∏è‚É£ Compute IV for all scorecard variables\n",
    "\n",
    "features_to_evaluate = [\n",
    "    \"EXT_SOURCE_2_BIN\",\n",
    "    \"EXT_SOURCE_3_BIN\",\n",
    "    \"AMT_CREDIT_BIN\",\n",
    "    \"AMT_GOODS_PRICE_BIN\",\n",
    "    \"NAME_INCOME_TYPE\",\n",
    "    \"NAME_EDUCATION_TYPE\",\n",
    "    \"OCCUPATION_TYPE\"\n",
    "]\n",
    "\n",
    "iv_results = {}\n",
    "\n",
    "for feature in features_to_evaluate:\n",
    "    _, iv_value = compute_woe_iv(df_sc, feature)\n",
    "    iv_results[feature] = iv_value\n",
    "\n",
    "iv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2cec2f-073b-48b0-aacc-d1ce44d8c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7Ô∏è‚É£ Inspect WOE table for EXT_SOURCE_3_BIN\n",
    "\n",
    "woe_ext3, iv_ext3 = compute_woe_iv(df_sc, \"EXT_SOURCE_3_BIN\")\n",
    "\n",
    "woe_ext3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09659c35-7afb-4448-9ac0-2437475bbfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8Ô∏è‚É£ Sort IV results from strongest to weakest\n",
    "\n",
    "iv_sorted = dict(sorted(iv_results.items(), key=lambda x: x[1], reverse=True))\n",
    "iv_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b588224-10aa-4aa1-b784-eb9f5fc6b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9Ô∏è‚É£ Create WOE mapping dictionary\n",
    "\n",
    "woe_maps = {}\n",
    "\n",
    "for feature in features_to_evaluate:\n",
    "    woe_table, _ = compute_woe_iv(df_sc, feature)\n",
    "    woe_maps[feature] = woe_table[\"woe\"].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409c804-ae97-4773-9586-2307efcc99b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîü Transform variables into WOE values\n",
    "\n",
    "df_woe = df_sc.copy()\n",
    "\n",
    "for feature in features_to_evaluate:\n",
    "    df_woe[feature + \"_WOE\"] = df_sc[feature].map(woe_maps[feature])\n",
    "\n",
    "# Select only WOE columns + target\n",
    "woe_columns = [f + \"_WOE\" for f in features_to_evaluate]\n",
    "\n",
    "df_woe_model = df_woe[woe_columns + [\"TARGET\"]].copy()\n",
    "\n",
    "df_woe_model.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee3dae-1a1c-4154-a20f-6dc1ae19baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_woe_model.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13db6f7-c7fc-470c-9a59-6304e07b6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£1Ô∏è‚É£ Prepare X (WOE features) and y (target)\n",
    "\n",
    "X = df_woe_model.drop(columns=[\"TARGET\"])\n",
    "y = df_woe_model[\"TARGET\"].astype(int)\n",
    "\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70311562-a229-456c-bae8-42b43da02f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£2Ô∏è‚É£ Train/validation split (stratified)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_valid.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6d787-6464-45cb-857e-ccffeaae4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£3Ô∏è‚É£ Train logistic regression on WOE features (scorecard model)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "woe_logit = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    class_weight=\"balanced\",\n",
    "    solver=\"lbfgs\"\n",
    ")\n",
    "\n",
    "woe_logit.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c087e20-d807-4416-bedb-543501b3a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£4Ô∏è‚É£ Evaluate scorecard model (ROC-AUC and PR-AUC)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "p_valid = woe_logit.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "roc_auc = roc_auc_score(y_valid, p_valid)\n",
    "pr_auc = average_precision_score(y_valid, p_valid)\n",
    "\n",
    "roc_auc, pr_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2fe480-8eb7-41cd-94fc-79fa8159e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£5Ô∏è‚É£ Compute KS statistic (credit risk standard metric)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def ks_statistic(y_true, y_score, n_bins=100):\n",
    "    data = pd.DataFrame({\"y\": y_true, \"score\": y_score}).sort_values(\"score\")\n",
    "    data[\"bin\"] = pd.qcut(data[\"score\"], q=n_bins, duplicates=\"drop\")\n",
    "    \n",
    "    grouped = data.groupby(\"bin\", observed=False)[\"y\"]\n",
    "    bad_rate = grouped.mean()\n",
    "    total = grouped.size()\n",
    "    \n",
    "    bad_cum = (bad_rate * total).cumsum() / (data[\"y\"].sum())\n",
    "    good_cum = ((1 - bad_rate) * total).cumsum() / ((1 - data[\"y\"]).sum())\n",
    "    \n",
    "    return np.max(np.abs(bad_cum - good_cum))\n",
    "\n",
    "ks = ks_statistic(y_valid, p_valid)\n",
    "ks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf598349-0924-4946-97d4-2f1d5b6c0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£6Ô∏è‚É£ Inspect scorecard model coefficients (interpretation)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"coefficient\": woe_logit.coef_[0],\n",
    "    \"odds_ratio\": np.exp(woe_logit.coef_[0])\n",
    "}).sort_values(\"coefficient\", ascending=False)\n",
    "\n",
    "coef_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea7c50-1149-44bd-9fd3-fb0579d6a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£7Ô∏è‚É£ Choose score scaling parameters (Base Score + PDO)\n",
    "\n",
    "# Base score: score at a chosen odds level (e.g., odds = 50:1 means PD ~ 1.96%)\n",
    "BASE_SCORE = 600\n",
    "BASE_ODDS = 50   # good:bad odds (50 means 50 non-default for 1 default)\n",
    "\n",
    "# Points to Double the Odds (PDO): +PDO points halves default odds\n",
    "PDO = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d4198-ca31-437d-8157-b9b3f507c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£8Ô∏è‚É£ Convert scaling parameters into A and B for: Score = A - B * log_odds\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "B = PDO / np.log(2)                # scaling factor\n",
    "A = BASE_SCORE + B * np.log(BASE_ODDS)  # offset\n",
    "\n",
    "A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379615d8-e370-4fe7-9045-7f2d76c4b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£9Ô∏è‚É£ Compute log-odds, PD, and credit score on the validation set\n",
    "\n",
    "# log-odds from logistic regression: log(p/(1-p)) = intercept + X*beta\n",
    "log_odds_valid = woe_logit.intercept_[0] + np.dot(X_valid.values, woe_logit.coef_[0])\n",
    "\n",
    "# Convert log-odds to PD\n",
    "pd_valid = 1 / (1 + np.exp(-log_odds_valid))\n",
    "\n",
    "# Convert log-odds to credit score (higher = lower risk)\n",
    "score_valid = A - B * log_odds_valid\n",
    "\n",
    "pd_valid[:5], score_valid[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f0299-031c-4970-95d6-544305d7b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£0Ô∏è‚É£ Put results into a nice dataframe (score + PD + actual target)\n",
    "\n",
    "valid_scored = X_valid.copy()\n",
    "valid_scored[\"PD\"] = pd_valid\n",
    "valid_scored[\"SCORE\"] = score_valid\n",
    "valid_scored[\"TARGET\"] = y_valid.values\n",
    "\n",
    "valid_scored[[\"PD\", \"SCORE\", \"TARGET\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3b56d6-706b-4a9c-9e01-9ee4aa680baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£1Ô∏è‚É£ Check score summary and default rate by score decile\n",
    "\n",
    "valid_scored[\"score_decile\"] = pd.qcut(valid_scored[\"SCORE\"], 10, duplicates=\"drop\")\n",
    "\n",
    "decile_summary = valid_scored.groupby(\"score_decile\", observed=False).agg(\n",
    "    n=(\"TARGET\", \"count\"),\n",
    "    default_rate=(\"TARGET\", \"mean\"),\n",
    "    avg_score=(\"SCORE\", \"mean\"),\n",
    "    avg_pd=(\"PD\", \"mean\")\n",
    ").sort_index(ascending=False)\n",
    "\n",
    "decile_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39015d8f-f08f-4f87-ba34-d7d1e4653a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£2Ô∏è‚É£ Plot score distribution for goods vs bads\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "PROJECT_DIR = Path(\"~/Documents/credit-scoring-home-credit\").expanduser()\n",
    "PLOTS_DIR = PROJECT_DIR / \"plots\"\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PLOTS_DIR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(valid_scored.loc[valid_scored[\"TARGET\"] == 0, \"SCORE\"], bins=50, alpha=0.6, label=\"Non-default (0)\")\n",
    "plt.hist(valid_scored.loc[valid_scored[\"TARGET\"] == 1, \"SCORE\"], bins=50, alpha=0.6, label=\"Default (1)\")\n",
    "plt.xlabel(\"Credit Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Score Distribution (Validation)\")\n",
    "plt.legend()\n",
    "plt.savefig(PLOTS_DIR / \"score_distribution_validation.png\", dpi=300, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b51f2-0cc3-4c73-ab2d-2124f9dcef68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (creditrisk311)",
   "language": "python",
   "name": "creditrisk311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
